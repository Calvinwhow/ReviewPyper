{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Provide Path to your API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key_path = r'/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/Software/Research/nimlab/openai_key.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - This will accept the .txt file of abstracts generated from a PubMed search and group every result into a CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class AbstractSeparator:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            self.content = file.read()\n",
    "        self.abstracts = []\n",
    "    \n",
    "    def separate_abstracts(self):\n",
    "        \"\"\"Separate the content into individual abstracts based on the described pattern.\"\"\"\n",
    "        abstract_entries = re.finditer(r'\\n(\\d+\\.\\s)', self.content)\n",
    "        start_positions = [match.start() for match in abstract_entries]\n",
    "        \n",
    "        # Create abstract chunks based on the start positions\n",
    "        abstract_chunks = [self.content[start_positions[i]:start_positions[i + 1]].strip() for i in range(len(start_positions) - 1)]\n",
    "        abstract_chunks.append(self.content[start_positions[-1]:].strip())\n",
    "        \n",
    "        self.abstracts = abstract_chunks\n",
    "    \n",
    "    def to_csv(self, output_path):\n",
    "        \"\"\"Save the separated abstracts to a CSV.\"\"\"\n",
    "        df = pd.DataFrame(self.abstracts, columns=[\"Abstract\"])\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "    def get_abstracts(self):\n",
    "        \"\"\"Return the list of separated abstracts.\"\"\"\n",
    "        return self.abstracts\n",
    "\n",
    "# Example usage:\n",
    "# separator = AbstractSeparator(\"/path/to/your/textfile.txt\")\n",
    "# separator.separate_abstracts()\n",
    "# separator.to_csv(\"/path/to/save/csvfile.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Only Evaluate Abstracts with Positive Title Hits (from notebook 00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleReviewFilter:\n",
    "    \"\"\"\n",
    "    A class to filter abstracts based on title review results.\n",
    "\n",
    "    Methods:\n",
    "    - load_data: Loads the title review results and abstracts data.\n",
    "    - filter_abstracts: Filters the abstracts based on a specified column from the title review results.\n",
    "    - save_filtered_data: Saves the filtered abstracts to a specified path.\n",
    "    - get_filtered_dataframe: Returns the filtered abstracts dataframe for visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, title_review_path, abstracts_path):\n",
    "        \"\"\"\n",
    "        Initializes the TitleReviewFilter class with paths to the title review results and abstracts CSVs.\n",
    "\n",
    "        Parameters:\n",
    "        - title_review_path (str): Path to the title review results CSV.\n",
    "        - abstracts_path (str): Path to the abstracts CSV.\n",
    "        \"\"\"\n",
    "        self.title_review_path = title_review_path\n",
    "        self.abstracts_path = abstracts_path\n",
    "        self.title_df, self.abstracts_df = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the title review results and abstracts data from CSVs.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame, DataFrame: DataFrames containing the title review results and abstracts.\n",
    "        \"\"\"\n",
    "        title_df = pd.read_csv(self.title_review_path)\n",
    "        abstracts_df = pd.read_csv(self.abstracts_path)\n",
    "        return title_df, abstracts_df\n",
    "\n",
    "    def filter_abstracts(self, column_name):\n",
    "        \"\"\"\n",
    "        Filters the abstracts based on a specified column from the title review results.\n",
    "\n",
    "        Parameters:\n",
    "        - column_name (str): The column name in the title review results to use for filtering.\n",
    "        \"\"\"\n",
    "        # Find the indices of the rows in title review results where the specified column has a value of 1\n",
    "        mask_indices = self.title_df[self.title_df[column_name] == 1].index\n",
    "        # Filter the abstracts dataframe using the mask indices\n",
    "        self.filtered_df = self.abstracts_df.iloc[mask_indices]\n",
    "\n",
    "    def save_filtered_data(self, output_path):\n",
    "        \"\"\"\n",
    "        Saves the filtered abstracts to a specified path.\n",
    "\n",
    "        Parameters:\n",
    "        - output_path (str): Path to save the filtered abstracts CSV.\n",
    "        \"\"\"\n",
    "        if not output_path.endswith('.csv'):\n",
    "            output_path += '.csv'\n",
    "        self.filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "    def get_filtered_dataframe(self):\n",
    "        \"\"\"\n",
    "        Returns the filtered abstracts dataframe for visualization.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: DataFrame containing the filtered abstracts.\n",
    "        \"\"\"\n",
    "        return self.filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths and column name\n",
    "title_review_path = \"/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/lnm_brain_death/systematic_review/raw/review_results.csv\"\n",
    "abstracts_path = \"/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/lnm_brain_death/systematic_review/raw/separated_abstracts.csv\"\n",
    "column_name_to_filter = \"Passes Title Screen (Very Sensitive)\"\n",
    "output_path_for_filtered_data = \"/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/lnm_brain_death/systematic_review/sensitive_masked_abstracts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create an instance of the TitleReviewFilter class\n",
    "title_review_filter = TitleReviewFilter(title_review_path, abstracts_path)\n",
    "\n",
    "# Filter the abstracts based on the specified column\n",
    "title_review_filter.filter_abstracts(column_name_to_filter)\n",
    "\n",
    "# Save the filtered data to a specified path (Optional)\n",
    "title_review_filter.save_filtered_data(output_path_for_filtered_data)\n",
    "\n",
    "# Get the filtered dataframe for visualization (Optional)\n",
    "filtered_df = title_review_filter.get_filtered_dataframe()\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Evaluate Abstracts That Passed Title Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "class AbstractEvaluatorDocumented:\n",
    "    \"\"\"\n",
    "    A class to evaluate abstracts from a CSV using the OpenAI API based on a posed question.\n",
    "\n",
    "    Attributes:\n",
    "    - api_key (str): OpenAI API key.\n",
    "    - df (DataFrame): DataFrame containing the abstracts to be evaluated.\n",
    "\n",
    "    Methods:\n",
    "    - read_api_key: Reads the OpenAI API key from a file.\n",
    "    - evaluate_with_openai: Evaluates an abstract based on the posed question using the OpenAI API.\n",
    "    - evaluate_abstracts: Evaluates all abstracts in the DataFrame based on the posed question.\n",
    "    - to_csv: Saves the updated DataFrame with the evaluation results to a CSV file.\n",
    "    - get_dataframe: Returns the updated DataFrame with the evaluation results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key_path, csv_path, organization_id=None):\n",
    "        \"\"\"\n",
    "        Initializes the AbstractEvaluatorDocumented class with the path to the API key and the CSV containing the abstracts.\n",
    "\n",
    "        Parameters:\n",
    "        - api_key_path (str): Path to the file containing the OpenAI API key.\n",
    "        - csv_path (str): Path to the CSV containing the abstracts.\n",
    "        \"\"\"\n",
    "        self.api_key = self.read_api_key(api_key_path)\n",
    "        openai.api_key = self.api_key\n",
    "        self.organization_id = organization_id\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "    \n",
    "    def read_api_key(self, file_path):\n",
    "        \"\"\"\n",
    "        Reads the OpenAI API key from a file.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path (str): Path to the file containing the OpenAI API key.\n",
    "\n",
    "        Returns:\n",
    "        - str: OpenAI API key.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.readline().strip()\n",
    "    \n",
    "    def evaluate_with_openai(self, abstract, question):\n",
    "        \"\"\"\n",
    "        Evaluates an abstract based on the posed question using the OpenAI API.\n",
    "\n",
    "        Parameters:\n",
    "        - abstract (str): The abstract to be evaluated.\n",
    "        - question (str): The posed question for evaluation.\n",
    "\n",
    "        Returns:\n",
    "        - int: Binary decision (0 or 1) based on the evaluation.\n",
    "        \"\"\"\n",
    "        headers = {}\n",
    "        if self.organization_id:\n",
    "            headers['OpenAI-Organization'] = self.organization_id\n",
    "            \n",
    "        prompt = f\"Abstract: {abstract}\\n{question}\\n\\nResponse (0 for No, 1 for Yes):\"\n",
    "        \n",
    "        retries = 3\n",
    "        for _ in range(retries):\n",
    "            try:\n",
    "                response = openai.Completion.create(\n",
    "                                                    engine=\"davinci\",\n",
    "                                                    prompt=prompt,\n",
    "                                                    max_tokens=10,\n",
    "                                                    headers=headers\n",
    "                                                )\n",
    "                decision_text = response.choices[0].text.strip()\n",
    "                decision = 1 if \"1\" in decision_text else 0\n",
    "                \n",
    "                return decision\n",
    "            except openai.error.OpenAIError as e:\n",
    "                if \"maximum context length\" in str(e):\n",
    "                    return -1\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                if _ < retries - 1:  # i.e. not on the last try yet\n",
    "                    time.sleep(2)  # wait for 2 seconds before trying again\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "\n",
    "    def evaluate_abstracts(self, question):\n",
    "        \"\"\"\n",
    "        Evaluates all abstracts in the DataFrame based on the posed question.\n",
    "\n",
    "        Parameters:\n",
    "        - question (str): The posed question for evaluation.\n",
    "        \"\"\"\n",
    "        tqdm.pandas()\n",
    "        self.df[\"Evaluation_Result\"] = self.df[\"Abstract\"].progress_apply(lambda abstract: self.evaluate_with_openai(abstract, question))\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    def to_csv(self, output_path):\n",
    "        \"\"\"\n",
    "        Saves the updated DataFrame with the evaluation results to a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "        - output_path (str): Path to save the CSV file.\n",
    "        \"\"\"\n",
    "        if not output_path.endswith('.csv'):\n",
    "            output_path += '.csv'\n",
    "        self.df.to_csv(output_path, index=False)\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Returns the updated DataFrame with the evaluation results.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Updated DataFrame containing the evaluation results.\n",
    "        \"\"\"\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_path = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/lnm_brain_death/systematic_review/sensitive_masked_abstracts.csv'\n",
    "question_to_gpt = \"Do you think this paper has at least one case of a focal lesion causing brain death?\"\n",
    "save_path = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/lnm_brain_death/systematic_review/filtered/sensitive_abstract_results.csv'\n",
    "organization_id = 'org-Y2tKyCPFO6tIjtCtOVZ7c9tr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (commented out for reference):\n",
    "evaluator = AbstractEvaluatorDocumented(openai_key_path, abstracts_path, organization_id)\n",
    "evaluator.evaluate_abstracts(question_to_gpt)\n",
    "results_df = evaluator.get_dataframe()\n",
    "evaluator.to_csv(save_path)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Positive Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Found {results_df[\"Evaluation_Result\"].sum()} positive abstracts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.to_csv(\"/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/lnm_brain_death/systematic_review/screened_abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
